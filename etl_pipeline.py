from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_timestamp
import time
import psutil  # Ensure this is installed in ray_env
import os

# âœ… Initialize Spark Session
spark = SparkSession.builder \
    .appName("ETL Benchmarking") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

# âœ… Define log file
LOG_FILE = "etl_benchmark.log"

# âœ… Start time measurement
start_time = time.time()

# âœ… Track memory usage before loading data
memory_before = psutil.virtual_memory().used / (1024 ** 3)  # GB

# âœ… Read all Parquet chunks from the HDFS directory
print("â³ Loading Parquet chunks from HDFS...")
parquet_path = "hdfs:///datasets/parquet_chunks/"  # Change path here
df = spark.read.parquet(parquet_path)

# âœ… Track memory usage after loading data
memory_after = psutil.virtual_memory().used / (1024 ** 3)  # GB

# âœ… Log dataset info
print(f"âœ… Loaded dataset with {df.count()} rows and {len(df.columns)} columns.")

# âœ… Show schema for debugging
df.printSchema()

# âœ… Perform ETL operations
print("ğŸ”„ Running ETL transformations...")

df_transformed = df \
    .withColumnRenamed("name", "full_name") \
    .withColumn("signup_date", to_timestamp(col("signup_date"), "yyyy-MM-dd HH:mm:ss"))

# âœ… Save transformed data back to HDFS
output_parquet_path = "hdfs:///datasets/processed_large_dataset.parquet"
df_transformed.write.mode("overwrite").parquet(output_parquet_path)

# âœ… Measure execution time
execution_time = time.time() - start_time

# âœ… Write benchmark log
with open(LOG_FILE, "a") as log_file:
    log_file.write("ğŸš€ ETL Process Benchmark\n")
    log_file.write(f"ğŸ”¹ Rows Processed: {df.count()}\n")
    log_file.write(f"ğŸ”¹ Columns: {len(df.columns)}\n")
    log_file.write(f"â³ Execution Time: {execution_time:.2f} seconds\n")
    log_file.write(f"ğŸ’¾ Memory Usage Before: {memory_before:.2f} GB\n")
    log_file.write(f"ğŸ’¾ Memory Usage After: {memory_after:.2f} GB\n")
    log_file.write("=" * 40 + "\n")

print(f"âœ… ETL process completed in {execution_time:.2f} seconds.")
print(f"ğŸ“‚ Processed dataset saved at: {output_parquet_path}")
print(f"ğŸ“œ Benchmark log saved in: {LOG_FILE}")

# âœ… Stop Spark session
spark.stop()
